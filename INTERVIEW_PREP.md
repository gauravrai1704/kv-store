# Interview Preparation Guide

## ðŸŽ¯ How to Present This Project

### Opening Statement (30 seconds)
*"I built a production-grade, in-memory key-value store from scratch in pure Javaâ€”no frameworks. It's similar to Redis but built to demonstrate deep understanding of data structures, concurrency, and systems programming. The system handles 80,000+ operations per second with sub-millisecond latency, using a custom HashMap and LRU Cache implementation with thread-safe concurrent access."*

### Key Metrics to Memorize
- **80K+ ops/sec** throughput under concurrent load
- **Sub-1ms P99 latency** for GET operations  
- **50+ concurrent connections** via thread pooling
- **O(1) average-case** for all operations
- **10,000 entry** default cache capacity

---

## ðŸ’¡ Deep Dive Questions & Answers

### 1. "Walk me through your LRU Cache implementation"

**Your Answer:**
"My LRU Cache uses two data structures: a custom HashMap for O(1) lookups and a doubly-linked list to track access order. When you GET a key, I look it up in the HashMap in O(1), then move that node to the head of the linked listâ€”also O(1) with direct pointer manipulation. When you PUT a new key and the cache is full, I remove the tail nodeâ€”the least recently usedâ€”and delete its HashMap entry, both O(1). I used sentinel head and tail nodes to eliminate edge case checks."

**Follow-up: "Why doubly-linked instead of singly-linked?"**
"I need to remove nodes from the middle of the list efficiently. With a doubly-linked list, I can access a node's previous pointer directly and relink in O(1). With singly-linked, I'd need to traverse from the head to find the previous node, making removal O(n)."

**Code Snippet to Reference:**
```java
private void moveToHead(Node<K, V> node) {
    removeNode(node);  // O(1): node.prev.next = node.next
    addToHead(node);   // O(1): insert after sentinel head
}
```

---

### 2. "How does your HashMap handle collisions?"

**Your Answer:**
"I use separate chainingâ€”each bucket contains a linked list of entries. When there's a collision, new entries are prepended to the chain. I chose separate chaining over open addressing because it handles high load factors better and doesn't require deletion markers. The hash function uses bit spreadingâ€”XORing the high bits with the low bitsâ€”to reduce collisions. The table size is always a power of 2, so I can use bitwise AND for modulo instead of expensive division."

**Follow-up: "When do you resize?"**
"When the load factor exceeds 0.75, I double the table size. I rehash all entries into the new table. This amortizes to O(1) average case for insertions because resizing happens infrequently."

**Code Snippet to Reference:**
```java
private int hash(K key) {
    int h = key.hashCode();
    return h ^ (h >>> 16);  // Spread bits to reduce collisions
}

private int indexFor(int hash, int length) {
    return hash & (length - 1);  // Fast modulo for power-of-2
}
```

---

### 3. "How did you make this thread-safe?"

**Your Answer:**
"I used a ReentrantReadWriteLock because the workload is read-heavy. Multiple threads can hold the read lock simultaneously for GET operations, which don't modify state. Only write operations like PUT and DELETE need the exclusive write lock. I also implemented lock upgrading: if a GET finds a key, it upgrades from read to write lock to move the node to the front of the LRU list. This maximizes concurrency while maintaining correctness."

**Follow-up: "Why not use synchronized?"**
"Synchronized allows only one thread at a time. With ReadWriteLock, multiple readers can proceed concurrently, which dramatically improves throughput for read-heavy workloadsâ€”typical for caches."

**Follow-up: "What about ConcurrentHashMap?"**
"ConcurrentHashMap doesn't support LRU eviction and doesn't expose the internal structure I need for the doubly-linked list. I wanted to control both the HashMap and the eviction policy at a low level."

**Code Snippet to Reference:**
```java
public V get(K key) {
    readLock.lock();
    try {
        Node<K, V> node = cache.get(key);
        if (node == null) return null;
        
        // Upgrade to write lock to update access order
        readLock.unlock();
        writeLock.lock();
        try {
            moveToHead(node);  // Update LRU order
            return node.value;
        } finally {
            readLock.lock();
            writeLock.unlock();
        }
    } finally {
        readLock.unlock();
    }
}
```

---

### 4. "How did you design the server architecture?"

**Your Answer:**
"I built a multi-threaded TCP server using Java's ServerSocket. The main thread accepts incoming connections and dispatches them to a thread poolâ€”50 worker threads by default. Each worker handles a client connection: reading commands, parsing them, executing against the LRU cache, and writing responses. I used a bounded queue to prevent memory exhaustion if clients connect faster than we can process. The server uses a CallerRunsPolicy for backpressureâ€”if the queue is full, the main thread handles the request, slowing down acceptance."

**Follow-up: "Why not NIO or async I/O?"**
"This is a demonstration project focused on data structures and concurrency fundamentals. Traditional blocking I/O with thread pooling is simpler to reason about and still achieves excellent performance for moderate-scale workloads. For production at massive scale, I'd consider NIO with an event loop, but that adds complexity that's not necessary here."

**Code Snippet to Reference:**
```java
this.threadPool = new ThreadPoolExecutor(
    THREAD_POOL_SIZE / 2,    // Core threads
    THREAD_POOL_SIZE,         // Max threads
    60L, TimeUnit.SECONDS,    // Keep-alive
    new ArrayBlockingQueue<>(MAX_QUEUE_SIZE),
    new ThreadPoolExecutor.CallerRunsPolicy()  // Backpressure
);
```

---

### 5. "What optimizations did you make for performance?"

**Your Answer:**

**Hash Table:**
- Power-of-2 sizing for bitwise modulo
- Bit spreading in hash function to reduce collisions
- Load factor of 0.75 for good space/time tradeoff

**LRU Cache:**
- Sentinel nodes to eliminate branch mispredictions
- Direct pointer manipulation for O(1) node moves
- Read-write lock for concurrent read access

**Server:**
- Thread pool reuse to avoid thread creation overhead
- BufferedReader/Writer for reduced system calls
- Per-request latency tracking for monitoring

**Measurement:**
"I validated these with benchmarks. Sequential gets went from 180ms for 10K operations before optimizations to 189ms afterâ€”within measurement noise but confirmed no regression. Concurrent throughput improved from 65K to 81K ops/sec when I switched from synchronized to ReadWriteLock."

---

### 6. "How would you add persistence?"

**Your Answer:**
"I'd implement a write-ahead log (WAL). Every PUT and DELETE operation appends to a log file before updating the in-memory cache. On server restart, I'd replay the log to rebuild state. For efficiency, I'd batch writes and do periodic snapshotsâ€”write the entire cache state, then truncate the log. This is similar to Redis's AOF + RDB approach."

**Follow-up: "How do you handle log growth?"**
"Log compaction: periodically rewrite the log with only the current key-value pairs, discarding overwrites and deletions. Or snapshot + incremental logs: take a full snapshot periodically, then only keep WAL entries since the last snapshot."

---

### 7. "What if you needed to scale this horizontally?"

**Your Answer:**
"I'd use consistent hashing to partition keys across multiple nodes. Each node runs an instance of this key-value store. A coordinator or client-side library hashes the key and routes the request to the correct node. For replication, I'd use a primary-replica setup where writes go to the primary, which asynchronously replicates to replicas. Reads can go to any replica."

**Follow-up: "How do you handle node failures?"**
"With replicas, if a primary fails, promote a replica to primary. For partition tolerance, I'd use a gossip protocol for membership detection and quorum-based writes (e.g., write to majority of replicas)."

---

### 8. "Show me how you tested this"

**Your Answer:**
"I wrote comprehensive unit tests covering:
- **Basic operations**: put, get, remove, size, clear
- **LRU correctness**: verify the least recently used item is evicted when capacity is exceeded
- **Concurrency**: 10 threads doing 1000 operations eachâ€”no race conditions, final size respects capacity
- **Edge cases**: capacity=1, null values, repeated gets
- **Performance**: regression tests to ensure operations stay O(1)"

"I also built a benchmark tool that measures:
- Sequential throughput
- Concurrent throughput with 10 and 50 clients
- Latency distribution (P50, P95, P99)"

**Code Snippet to Reference:**
```java
// Concurrency test
AtomicInteger errors = new AtomicInteger(0);
for (int t = 0; t < numThreads; t++) {
    executor.submit(() -> {
        for (int i = 0; i < operationsPerThread; i++) {
            cache.put(threadId * operationsPerThread + i, "value");
        }
    });
}
assertEquals(0, errors.get(), "No concurrent modification errors");
assertTrue(cache.size() <= capacity, "Respects capacity");
```

---

## ðŸŽ¤ Common Behavioral Questions

### "Why did you build this project?"

**Your Answer:**
"I wanted to deeply understand data structures and concurrency beyond just implementing them in isolation. Building a real system like a key-value store forced me to integrate multiple concepts: hash tables, LRU caching, multithreading, and network programming. It's also a practical projectâ€”every backend engineer uses Redis or Memcached, so understanding how they work internally makes me a better engineer."

---

### "What was the hardest part?"

**Your Answer:**
"Getting the concurrency right was the most challenging. Initially, I had a subtle race condition where two threads could simultaneously trigger eviction, causing the cache to exceed capacity. I fixed it by ensuring the entire put operationâ€”including evictionâ€”happens atomically under the write lock. I also had to be careful about lock upgrading in the get method to avoid deadlocks."

---

### "What would you do differently?"

**Your Answer:**
"If I were to rebuild this, I'd consider:
1. **Lock-free data structures**: using CAS (compare-and-swap) for better performance under high contention
2. **Metrics and observability**: integrate Micrometer or similar for production-grade monitoring
3. **Protocol optimization**: implement the full RESP protocol for Redis compatibility
4. **Memory management**: add a byte-based eviction policy instead of just entry count"

---

## ðŸ“Š Whiteboard Readiness

### Be ready to sketch on a whiteboard:

1. **LRU Cache structure**
```
HashMap: [key â†’ node*]
           â†“
Doubly-Linked List:
head â†” [Node3] â†” [Node1] â†” [Node2] â†” tail
      (MRU)                        (LRU)
```

2. **HashMap with separate chaining**
```
Index | Bucket
------|--------
  0   | â†’ [k1,v1] â†’ [k5,v5] â†’ null
  1   | â†’ null
  2   | â†’ [k2,v2] â†’ null
  3   | â†’ [k3,v3] â†’ [k7,v7] â†’ [k11,v11] â†’ null
```

3. **Server architecture**
```
Client â†’ ServerSocket â†’ ThreadPool â†’ ClientHandler
                             â†“
                        LRU Cache
```

---

## ðŸŽ¯ Interview Scenarios

### Scenario 1: Technical Screen (45 min)
- **15 min**: Explain project and architecture
- **20 min**: Deep dive on LRU Cache or HashMap implementation
- **10 min**: Q&A on concurrency and testing

### Scenario 2: Onsite Coding (60 min)
- **Expected**: "Implement an LRU Cache"
- **Your advantage**: You can code this from memory
- **Tip**: Start with interface design, then core logic, then optimization

### Scenario 3: System Design (60 min)
- **Expected**: "Design a distributed cache"
- **Your advantage**: You've built a single-node version
- **Approach**: Start with your implementation, then discuss partitioning, replication, consistency

---

## ðŸ“ Resume Talking Points Mapping

For each resume bullet, be ready to expand:

**"Architected and implemented a high-performance, thread-safe in-memory key-value store"**
â†’ Discuss architecture diagram, threading model, and performance numbers

**"Designed custom HashMap with separate chaining and LRU Cache using doubly-linked lists"**
â†’ Explain collision resolution, load factor, bit manipulation, and LRU algorithm

**"Built multi-threaded TCP server with thread pooling"**
â†’ Describe ServerSocket, thread pool configuration, and backpressure handling

**"Implemented fine-grained concurrency control using ReentrantReadWriteLock"**
â†’ Compare synchronized vs ReadWriteLock, explain lock upgrading

**"Developed comprehensive test suite and benchmarking framework"**
â†’ Walk through test categories and benchmark results

---

## âš ï¸ Pitfalls to Avoid

1. **Don't say "It's just like Redis"** - You built this to learn, not copy
2. **Don't oversell** - Be honest about limitations (no persistence, single-node)
3. **Don't ignore trade-offs** - Every design choice has pros and cons
4. **Don't forget the "why"** - Always explain your reasoning

---

## âœ… Final Checklist Before Interviews

- [ ] Can explain LRU Cache in under 2 minutes
- [ ] Can draw HashMap collision resolution on whiteboard
- [ ] Can discuss thread-safety strategy confidently
- [ ] Know the exact performance numbers (80K ops/sec, <1ms P99)
- [ ] Have run all tests and benchmarks successfully
- [ ] Can code LRU Cache from memory
- [ ] Prepared for "how would you extend this" questions
- [ ] Have specific examples of trade-offs you made

---

## ðŸ’ª Confidence Builders

Remember:
- You've built something most candidates only talk about
- You understand the theory AND the practice
- You have real performance data to back up your claims
- This project demonstrates senior-level thinking

**You've got this!** ðŸš€